{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "919bdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29b7287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set NLTK data path\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "os.environ['NLTK_DATA'] = nltk_data_path\n",
    "\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt', paths=[nltk_data_path])\n",
    "except LookupError:\n",
    "    nltk.download('punkt', download_dir=nltk_data_path, quiet=True)\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords', paths=[nltk_data_path])\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', download_dir=nltk_data_path, quiet=True)\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet', paths=[nltk_data_path])\n",
    "except LookupError:\n",
    "    nltk.download('wordnet', download_dir=nltk_data_path, quiet=True)\n",
    "\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71fa33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "bi_encoder = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02ec29ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0da40549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (as integers)\n",
    "TITLE_WEIGHT = 2  # Now integer\n",
    "SKILLS_WEIGHT = 2  # Now integer\n",
    "DESC_WEIGHT = 1    # Now integer\n",
    "ALPHA = 0.7\n",
    "MIN_SIMILARITY = 0.4\n",
    "RERANK_TOP_N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17aac838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text for better embeddings\"\"\"\n",
    "    if not text or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        # Lemmatization\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    except:\n",
    "        # Fallback to simple splitting\n",
    "        tokens = text.split()\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a76d3921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_text(item, item_type):\n",
    "    \"\"\"Create weighted text representation based on item type\"\"\"\n",
    "    if item_type == \"job\":\n",
    "        title = item.get('title', '')\n",
    "        description = item.get('description', '')\n",
    "        skills = item.get('skills', [])\n",
    "    else:  # service\n",
    "        title = item.get('title', '')\n",
    "        description = item.get('description', '')\n",
    "        skills = item.get('tags', [])  # Using tags for services\n",
    "    \n",
    "    # Create weighted representation\n",
    "    weighted_terms = []\n",
    "    \n",
    "    # Add title words multiple times based on weight\n",
    "    for word in title.split():\n",
    "        weighted_terms.extend([word] * TITLE_WEIGHT)\n",
    "    \n",
    "    # Add skills multiple times based on weight\n",
    "    for skill in skills:\n",
    "        weighted_terms.extend([skill] * SKILLS_WEIGHT)\n",
    "    \n",
    "    # Add description words\n",
    "    weighted_terms.extend(description.split())\n",
    "    \n",
    "    return \" \".join(weighted_terms)\n",
    "\n",
    "def extract_skills(text):\n",
    "    \"\"\"Simple skill extractor\"\"\"\n",
    "    skills = set()\n",
    "    # Look for capitalized words that look like skills\n",
    "    for word in re.findall(r'\\b[A-Z][a-z]{2,}\\b', text):\n",
    "        if word.lower() not in stop_words and len(word) > 3:\n",
    "            skills.add(word)\n",
    "    return list(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "568d401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(cv_text, jobs, services, top_n=5):\n",
    "    # 1. Preprocess CV and extract skills\n",
    "    cv_processed = preprocess_text(cv_text)\n",
    "    print(f\"Processed CV: {cv_processed[:200]}...\")\n",
    "    cv_skills = extract_skills(cv_text)\n",
    "    print(f\"Extracted CV skills: {cv_skills}\")\n",
    "    \n",
    "    # 2. Create job representations\n",
    "    job_texts = []\n",
    "    job_skill_lists = []\n",
    "    for job in jobs:\n",
    "        job_text = create_weighted_text(job, \"job\")\n",
    "        job_texts.append(job_text)\n",
    "        job_skill_lists.append(job.get('skills', []))\n",
    "    \n",
    "    # 3. Create service representations\n",
    "    service_texts = []\n",
    "    service_skill_lists = []\n",
    "    for service in services:\n",
    "        service_text = create_weighted_text(service, \"service\")\n",
    "        service_texts.append(service_text)\n",
    "        service_skill_lists.append(service.get('tags', []))\n",
    "    \n",
    "    # 4. Calculate skill match scores\n",
    "    job_skill_scores = []\n",
    "    for skills in job_skill_lists:\n",
    "        if not skills:\n",
    "            job_skill_scores.append(0)\n",
    "            continue\n",
    "        common = set(cv_skills) & set(skills)\n",
    "        job_skill_scores.append(len(common) / len(skills))\n",
    "    \n",
    "    service_skill_scores = []\n",
    "    for skills in service_skill_lists:\n",
    "        if not skills:\n",
    "            service_skill_scores.append(0)\n",
    "            continue\n",
    "        common = set(cv_skills) & set(skills)\n",
    "        service_skill_scores.append(len(common) / len(skills))\n",
    "    \n",
    "    # 5. TF-IDF Vectorization\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    all_texts = [cv_processed] + job_texts + service_texts\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(all_texts)\n",
    "    cv_tfidf = tfidf_matrix[0]\n",
    "    job_tfidfs = tfidf_matrix[1:1+len(jobs)]\n",
    "    service_tfidfs = tfidf_matrix[1+len(jobs):]\n",
    "    \n",
    "    # 6. Bi-encoder Embeddings\n",
    "    cv_embedding = bi_encoder.encode(cv_processed, convert_to_tensor=True)\n",
    "    job_embeddings = bi_encoder.encode(job_texts, convert_to_tensor=True)\n",
    "    service_embeddings = bi_encoder.encode(service_texts, convert_to_tensor=True)\n",
    "    \n",
    "    # 7. Calculate similarities\n",
    "    # Semantic similarities\n",
    "    job_scores_semantic = util.cos_sim(cv_embedding, job_embeddings)[0].cpu().numpy()\n",
    "    service_scores_semantic = util.cos_sim(cv_embedding, service_embeddings)[0].cpu().numpy()\n",
    "    \n",
    "    # Keyword similarities\n",
    "    job_scores_keyword = np.array([(cv_tfidf @ job_tfidfs[i].T).toarray()[0][0] \n",
    "                                 for i in range(len(jobs))])\n",
    "    service_scores_keyword = np.array([(cv_tfidf @ service_tfidfs[i].T).toarray()[0][0] \n",
    "                                     for i in range(len(services))])\n",
    "    \n",
    "    # Hybrid scores\n",
    "    job_scores_hybrid = (ALPHA * job_scores_semantic + \n",
    "                         (1 - ALPHA) * job_scores_keyword)\n",
    "    service_scores_hybrid = (ALPHA * service_scores_semantic + \n",
    "                             (1 - ALPHA) * service_scores_keyword)\n",
    "    \n",
    "    # Add skill bonus\n",
    "    job_scores_hybrid += 0.2 * np.array(job_skill_scores)\n",
    "    service_scores_hybrid += 0.2 * np.array(service_skill_scores)\n",
    "    \n",
    "    # 8. Cross-encoder re-ranking (for top candidates)\n",
    "    def rerank(candidates, texts, scores, top_n):\n",
    "        # Get top candidates for re-ranking\n",
    "        top_indices = np.argsort(-scores)[:RERANK_TOP_N]\n",
    "        pairs = [(cv_processed, texts[i]) for i in top_indices]\n",
    "        rerank_scores = cross_encoder.predict(pairs)\n",
    "        \n",
    "        # Update scores\n",
    "        updated_scores = scores.copy()\n",
    "        for idx, score in zip(top_indices, rerank_scores):\n",
    "            updated_scores[idx] = 0.7 * updated_scores[idx] + 0.3 * score\n",
    "        \n",
    "        # Get final top indices\n",
    "        final_indices = np.argsort(-updated_scores)[:top_n]\n",
    "        return [candidates[i]['id'] for i in final_indices \n",
    "                if updated_scores[i] > MIN_SIMILARITY]\n",
    "    \n",
    "    # 9. Get recommendations\n",
    "    recommended_job_ids = rerank(jobs, job_texts, job_scores_hybrid, top_n)\n",
    "    recommended_service_ids = rerank(services, service_texts, service_scores_hybrid, top_n)\n",
    "    \n",
    "    print(f\"Recommended job IDs: {recommended_job_ids}\")\n",
    "    print(f\"Recommended service IDs: {recommended_service_ids}\")\n",
    "    \n",
    "    return {\n",
    "        \"recommendedJobs\": recommended_job_ids,\n",
    "        \"recommendedServices\": recommended_service_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29dd7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "developer_cv = \"\"\"\n",
    "Experienced full-stack developer with expertise in React, Node.js, PostgreSQL, and cloud platforms.\n",
    "Passionate about scalable backend systems and responsive frontend interfaces. \n",
    "Worked on SaaS platforms and internal tools for data analysis.\n",
    "\"\"\"\n",
    "\n",
    "job_posts = [\n",
    "    {\n",
    "        \"id\": \"job1\",\n",
    "        \"title\": \"Full Stack Developer\",\n",
    "        \"description\": \"Looking for a React and Node.js developer to build scalable applications.\",\n",
    "        \"skills\": [\"React\", \"Node.js\", \"PostgreSQL\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"job2\",\n",
    "        \"title\": \"Backend Engineer\",\n",
    "        \"description\": \"Seeking expert in Python and Django for backend API development.\",\n",
    "        \"skills\": [\"Python\", \"Django\", \"APIs\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"job3\",\n",
    "        \"title\": \"Frontend Developer\",\n",
    "        \"description\": \"We need someone skilled in Vue.js and TailwindCSS for UI work.\",\n",
    "        \"skills\": [\"Vue.js\", \"TailwindCSS\", \"UI\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"job4\",\n",
    "        \"title\": \"Cloud Infrastructure Engineer\",\n",
    "        \"description\": \"Design and implement cloud solutions on AWS and Azure.\",\n",
    "        \"skills\": [\"AWS\", \"Azure\", \"Cloud\", \"DevOps\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "service_posts = [\n",
    "    {\n",
    "        \"id\": \"service1\",\n",
    "        \"title\": \"React Mentor\",\n",
    "        \"description\": \"Offering 1:1 React mentorship and live project reviews.\",\n",
    "        \"skills\": [\"React\", \"Mentorship\", \"Frontend\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"service2\",\n",
    "        \"title\": \"Database Consultant\",\n",
    "        \"description\": \"Helping startups optimize PostgreSQL queries and schema design.\",\n",
    "        \"skills\": [\"PostgreSQL\", \"Database\", \"Performance\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"service3\",\n",
    "        \"title\": \"DevOps Setup\",\n",
    "        \"description\": \"Set up CI/CD pipelines and Docker containers for fast deployment.\",\n",
    "        \"skills\": [\"DevOps\", \"CI/CD\", \"Docker\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"service4\",\n",
    "        \"title\": \"Node.js Performance Tuning\",\n",
    "        \"description\": \"Optimize your Node.js backend for high throughput and low latency.\",\n",
    "        \"skills\": [\"Node.js\", \"Performance\", \"Backend\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c723dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting recommendation...\n",
      "Processed CV: experienced fullstack developer with expertise in react nodejs postgresql and cloud platforms passionate about scalable backend systems and responsive frontend interfaces worked on saas platforms and ...\n",
      "Extracted CV skills: ['Worked', 'Experienced', 'React', 'Node', 'Passionate']\n",
      "Recommended job IDs: ['job1']\n",
      "Recommended service IDs: []\n",
      "\n",
      "Final Recommendations:\n",
      "{'recommendedJobs': ['job1'], 'recommendedServices': []}\n"
     ]
    }
   ],
   "source": [
    "# Run recommendation\n",
    "print(\"Starting recommendation...\")\n",
    "results = recommend(developer_cv, job_posts, service_posts, top_n=3)\n",
    "print(\"\\nFinal Recommendations:\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
